# ğŸš€ COMPLETE MLOps PROJECT - READY TO USE!

## âœ… What You Have

Your **COMPLETE** end-to-end MLOps project with:
- âœ… Data preprocessing pipeline
- âœ… Feature engineering
- âœ… Model training with MLflow
- âœ… Model evaluation and metrics
- âœ… Prediction module
- âœ… FastAPI REST API
- âœ… Docker containerization  
- âœ… Docker Compose with monitoring
- âœ… Prometheus + Grafana monitoring
- âœ… Drift detection with Evidently
- âœ… Airflow retraining pipeline
- âœ… GitHub Actions CI/CD
- âœ… Complete test suite
- âœ… Makefile for easy commands

**ALL CODE IS COMPLETE AND READY TO RUN!**

---

## ğŸ¯ Quick Start (5 Steps)

### **Step 1: Setup Environment**
```bash
cd ml-churn-prediction
source venv/bin/activate  # On Mac/Linux
pip install -r requirements.txt  # Takes 5-10 minutes
```

### **Step 2: Download Data**
```bash
make download-data
# OR
python src/utils/download_data.py
```

### **Step 3: Train Model**
```bash
make all
# This runs: preprocess â†’ train â†’ evaluate
```

### **Step 4: Start API**
```bash
make api
# API runs at http://localhost:8000
# Docs at http://localhost:8000/docs
```

### **Step 5: Test It!**
```bash
# Open http://localhost:8000/docs in browser
# Try the /predict endpoint with sample data
```

---

## ğŸ“š Complete Command Reference

### **Using Makefile (Easiest)**
```bash
make help              # Show all commands
make install           # Install dependencies
make download-data     # Download dataset
make preprocess        # Preprocess data
make train             # Train models
make evaluate          # Evaluate models
make predict           # Test predictions
make api               # Start API (dev mode)
make api-prod          # Start API (production)
make test              # Run tests
make format            # Format code
make lint              # Lint code
make docker            # Run with Docker Compose
make mlflow            # Start MLflow UI
make clean             # Clean temporary files
make all               # Run complete pipeline
```

### **Direct Python Commands**
```bash
# Data pipeline
python src/utils/download_data.py
python src/features/preprocessing.py
python src/models/train.py
python src/models/evaluate.py
python src/models/predict.py

# API
uvicorn src.api.main:app --reload

# Monitoring
python src/monitoring/drift_detection.py
```

### **Docker Commands**
```bash
# Build image
docker build -t churn-api .

# Run container
docker run -p 8000:8000 churn-api

# Run with compose (includes monitoring)
docker-compose up --build

# Stop containers
docker-compose down
```

### **Testing**
```bash
# Run all tests
pytest

# With coverage
pytest --cov=src --cov-report=html

# Specific test file
pytest tests/test_all.py -v

# Run fast tests only
pytest -m "not slow"
```

---

## ğŸƒ Complete Workflow

### **Development Workflow:**
```bash
# 1. Download data
make download-data

# 2. Run complete pipeline
make all
# This does: preprocess â†’ train â†’ evaluate

# 3. Start API
make api

# 4. Test in browser
# Open: http://localhost:8000/docs

# 5. Make predictions via API
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "gender": "Female",
    "SeniorCitizen": 0,
    "Partner": "Yes",
    "Dependents": "No",
    "tenure": 12,
    "PhoneService": "Yes",
    "MultipleLines": "No",
    "InternetService": "DSL",
    "OnlineSecurity": "Yes",
    "OnlineBackup": "Yes",
    "DeviceProtection": "No",
    "TechSupport": "No",
    "StreamingTV": "No",
    "StreamingMovies": "No",
    "Contract": "One year",
    "PaperlessBilling": "Yes",
    "PaymentMethod": "Electronic check",
    "MonthlyCharges": 50.5,
    "TotalCharges": 600.0
  }'
```

### **Production Workflow:**
```bash
# 1. Run tests
make test

# 2. Build Docker image
make docker-build

# 3. Run with monitoring
make docker

# 4. Access services:
# - API: http://localhost:8000
# - MLflow: http://localhost:5000
# - Prometheus: http://localhost:9090
# - Grafana: http://localhost:3000
```

---

## ğŸ“Š What Each Module Does

### **1. Data Preprocessing** (`src/features/preprocessing.py`)
- Loads raw data
- Cleans missing values
- Encodes categorical variables
- Scales numeric features
- Splits train/test sets
- **Run:** `make preprocess`

### **2. Feature Engineering** (`src/features/feature_engineering.py`)
- Creates tenure buckets
- Calculates charge ratios
- Counts services
- Creates customer segments
- **Used automatically** during preprocessing

### **3. Model Training** (`src/models/train.py`)
- Trains 3 models: Logistic Regression, Random Forest, XGBoost
- Tracks experiments with MLflow
- Saves best model
- **Run:** `make train`
- **View experiments:** `make mlflow` â†’ http://localhost:5000

### **4. Model Evaluation** (`src/models/evaluate.py`)
- Calculates metrics (accuracy, precision, recall, F1, ROC-AUC)
- Creates confusion matrix
- Plots ROC curve
- Shows feature importance
- **Run:** `make evaluate`
- **Results:** `data/models/evaluation/`

### **5. Model Prediction** (`src/models/predict.py`)
- Makes predictions on new data
- Returns probability and risk level
- Explains top influential features
- **Run:** `make predict`

### **6. FastAPI Application** (`src/api/main.py`)
- REST API for predictions
- Endpoints: `/predict`, `/predict/batch`, `/health`, `/model/info`
- Interactive docs at `/docs`
- **Run:** `make api`
- **Access:** http://localhost:8000/docs

### **7. Monitoring** (`src/monitoring/drift_detection.py`)
- Detects data drift with Evidently
- Prometheus metrics
- Performance tracking
- **Run:** `python src/monitoring/drift_detection.py`

### **8. Airflow DAG** (`airflow_dags/retraining_pipeline.py`)
- Automated retraining workflow
- Scheduled weekly (Sundays)
- Fetches â†’ Validates â†’ Trains â†’ Evaluates â†’ Deploys
- **Setup Airflow separately** (instructions below)

### **9. CI/CD Pipeline** (`.github/workflows/ci-cd.yml`)
- Runs tests on every push
- Builds Docker image
- Deploys to staging/production
- **Activates** when you push to GitHub

---

## ğŸ§ª Testing Your Project

### **1. Unit Tests**
```bash
make test
```

### **2. API Tests**
```bash
# Start API first
make api

# In another terminal, test endpoints
curl http://localhost:8000/health
curl http://localhost:8000/model/info

# Test prediction
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d @sample_request.json
```

### **3. Integration Test (Full Pipeline)**
```bash
# Run complete pipeline
make clean              # Clean old files
make download-data      # Download data
make all                # Preprocess, train, evaluate
make test               # Run tests
make api                # Start API

# Everything should work!
```

---

## ğŸ³ Docker Deployment

### **Option 1: Simple Docker**
```bash
# Build
docker build -t churn-api .

# Run
docker run -p 8000:8000 \
  -v $(pwd)/data:/app/data \
  -e ENVIRONMENT=production \
  churn-api

# Test
curl http://localhost:8000/health
```

### **Option 2: Docker Compose (with Monitoring)**
```bash
# Start everything
docker-compose up --build

# Services available:
# - API: http://localhost:8000
# - MLflow: http://localhost:5000  
# - Prometheus: http://localhost:9090
# - Grafana: http://localhost:3000 (admin/admin)

# Stop
docker-compose down
```

---

## ğŸ“ˆ Monitoring Setup

### **1. View MLflow Experiments**
```bash
make mlflow
# Open: http://localhost:5000
```

### **2. Check Prometheus Metrics**
```bash
# Start with docker-compose
docker-compose up prometheus

# Open: http://localhost:9090
```

### **3. View Grafana Dashboards**
```bash
# Start with docker-compose
docker-compose up grafana

# Open: http://localhost:3000
# Login: admin / admin
```

### **4. Drift Detection**
```bash
python src/monitoring/drift_detection.py
# Reports saved to: monitoring/reports/
```

---

## â˜ï¸ Cloud Deployment (Optional)

### **AWS Deployment**
1. Build and push Docker image to ECR
2. Deploy to ECS/EKS
3. Use Terraform (in `terraform/` folder)

### **GCP Deployment**
1. Build and push to Container Registry
2. Deploy to Cloud Run
3. Use Terraform

### **DigitalOcean/Heroku**
Simpler options for quick deployment

---

## ğŸ”„ Airflow Setup (Optional)

```bash
# Install Airflow
pip install apache-airflow

# Initialize database
airflow db init

# Create admin user
airflow users create \
  --username admin \
  --password admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com

# Start webserver
airflow webserver --port 8080

# Start scheduler (in another terminal)
airflow scheduler

# Access: http://localhost:8080
# Enable the "churn_model_retraining" DAG
```

---

## ğŸ“ Project Structure Explained

```
ml-churn-prediction/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ features/          # Data preprocessing & feature engineering
â”‚   â”‚   â”œâ”€â”€ preprocessing.py      â† Clean and transform data
â”‚   â”‚   â””â”€â”€ feature_engineering.py â† Create new features
â”‚   â”œâ”€â”€ models/            # Model training and prediction
â”‚   â”‚   â”œâ”€â”€ train.py              â† Train models with MLflow
â”‚   â”‚   â”œâ”€â”€ evaluate.py           â† Evaluate and visualize
â”‚   â”‚   â””â”€â”€ predict.py            â† Make predictions
â”‚   â”œâ”€â”€ api/               # FastAPI application
â”‚   â”‚   â””â”€â”€ main.py               â† REST API endpoints
â”‚   â”œâ”€â”€ monitoring/        # Monitoring and drift detection
â”‚   â”‚   â””â”€â”€ drift_detection.py    â† Monitor performance
â”‚   â””â”€â”€ utils/             # Utilities
â”‚       â”œâ”€â”€ config_loader.py      â† Load config
â”‚       â”œâ”€â”€ logger.py             â† Logging
â”‚       â””â”€â”€ download_data.py      â† Download dataset
â”œâ”€â”€ tests/                 # Test suite
â”‚   â””â”€â”€ test_all.py               â† All tests
â”œâ”€â”€ airflow_dags/          # Airflow workflows
â”‚   â””â”€â”€ retraining_pipeline.py    â† Automated retraining
â”œâ”€â”€ .github/workflows/     # CI/CD
â”‚   â””â”€â”€ ci-cd.yml                 â† GitHub Actions
â”œâ”€â”€ config/                # Configuration
â”‚   â””â”€â”€ config.yaml               â† All settings
â”œâ”€â”€ data/                  # Data directories
â”‚   â”œâ”€â”€ raw/                      â† Original data
â”‚   â”œâ”€â”€ processed/                â† Processed data
â”‚   â””â”€â”€ models/                   â† Saved models
â”œâ”€â”€ Dockerfile                    â† Container definition
â”œâ”€â”€ docker-compose.yml            â† Multi-container setup
â”œâ”€â”€ prometheus.yml                â† Monitoring config
â”œâ”€â”€ Makefile                      â† Easy commands
â”œâ”€â”€ requirements.txt              â† Dependencies
â””â”€â”€ README.md                     â† Main documentation
```

---

## ğŸ¯ Common Tasks

### **Retrain Model**
```bash
make download-data  # Get fresh data
make all            # Retrain everything
```

### **Deploy New Version**
```bash
make test           # Ensure tests pass
make docker-build   # Build image
make docker         # Deploy locally
# OR push to cloud
```

### **Check Model Performance**
```bash
make evaluate
# Check: data/models/evaluation/
```

### **Monitor Drift**
```bash
python src/monitoring/drift_detection.py
# Check: monitoring/reports/
```

---

## ğŸ› Troubleshooting

### **Issue: "Model not found"**
```bash
# Train the model first
make train
```

### **Issue: "Processed data not found"**
```bash
# Run preprocessing
make preprocess
```

### **Issue: "Module not found"**
```bash
# Make sure venv is activated
source venv/bin/activate

# Reinstall requirements
pip install -r requirements.txt
```

### **Issue: "Port already in use"**
```bash
# Check what's using port 8000
lsof -i :8000

# Kill process or use different port
uvicorn src.api.main:app --port 8001
```

---

## ğŸ‰ Next Steps

### **For Learning:**
1. âœ… Run complete pipeline: `make all`
2. âœ… Start API: `make api`
3. âœ… Explore MLflow: `make mlflow`
4. âœ… Run tests: `make test`
5. âœ… Try Docker: `make docker`

### **For Production:**
1. âœ… Set up monitoring (Prometheus + Grafana)
2. âœ… Configure Airflow for retraining
3. âœ… Set up GitHub Actions
4. âœ… Deploy to cloud
5. âœ… Add custom business logic

### **For Resume:**
1. âœ… Push to GitHub
2. âœ… Add screenshots/demo video
3. âœ… Write blog post explaining architecture
4. âœ… Showcase in portfolio

---

## ğŸ“Š Success Checklist

- [ ] Data downloaded
- [ ] Model trained (check `data/models/best_model.pkl`)
- [ ] Tests passing (`make test`)
- [ ] API running (`http://localhost:8000/docs`)
- [ ] Can make predictions via API
- [ ] MLflow tracking works
- [ ] Docker image builds
- [ ] Monitoring works

---

## ğŸ’¡ Key Files to Understand

**Start with these:**
1. `config/config.yaml` - All settings
2. `src/models/train.py` - Model training logic
3. `src/api/main.py` - API endpoints
4. `Makefile` - Available commands
5. `README.md` - Main documentation

**Advanced:**
6. `airflow_dags/retraining_pipeline.py` - Automation
7. `.github/workflows/ci-cd.yml` - CI/CD
8. `src/monitoring/drift_detection.py` - Monitoring
9. `docker-compose.yml` - Container orchestration

---

## ğŸš€ YOU'RE READY!

Everything is built and ready to use. Just run:

```bash
make all  # Complete pipeline
make api  # Start API
```

**ğŸŠ Congratulations! You have a production-ready MLOps system! ğŸŠ**

---

Questions? Issues? Check the README.md or ask for help!
